# ComfyUI pour RunPod avec CUDA 12.8 - Compatible RTX 5090

Ce dÃ©pÃ´t contient les fichiers nÃ©cessaires pour crÃ©er un conteneur Docker optimisÃ© pour exÃ©cuter ComfyUI sur la plateforme RunPod avec CUDA 12.8, spÃ©cialement configurÃ© pour les GPU NVIDIA de derniÃ¨re gÃ©nÃ©ration, y compris la RTX 5090.


## FonctionnalitÃ©s

- **Base CUDA 12.8** - Support complet des GPU de derniÃ¨re gÃ©nÃ©ration (RTX 5090, 4090, 4080, etc.)
- **Image NGC PyTorch 25.02** - Utilise l'image officielle NVIDIA NGC avec PyTorch pour des performances maximales
- **Architecture GPU Ã©tendue** - Support natif pour les compute capabilities 8.6, 9.0, 12.0 et 12.6
- **TÃ©lÃ©chargement automatique des modÃ¨les** - Les modÃ¨les sont tÃ©lÃ©chargÃ©s depuis Hugging Face au premier dÃ©marrage
- **Extensions prÃ©-installÃ©es** - ComfyUI-Manager, sdxl_prompt_styler, ComfyUI_TensorRT et plus
- **Interface web avec Nginx** - Configuration optimisÃ©e pour l'accÃ¨s Ã  distance
- **Support JupyterLab** - Pour le dÃ©veloppement et l'exploration des fonctionnalitÃ©s
- **Persistance des donnÃ©es** - Les modÃ¨les et les configurations sont conservÃ©s dans un volume network

## PrÃ©requis

- Docker installÃ© sur votre machine (pour la construction de l'image)
- Si vous utilisez un Mac avec puce Apple Silicon (M1/M2/M3), utilisez Docker Desktop avec support d'Ã©mulation
- Un compte RunPod avec des crÃ©dits ou un GPU disponible
- Un compte Hugging Face (pour le tÃ©lÃ©chargement des modÃ¨les)

## DÃ©marrage rapide

1. **Clonez ce dÃ©pÃ´t** :
```bash
git clone https://github.com/Seven-94/Runpod_ComfyUI.git
cd Runpod_ComfyUI
```

2. **Construisez l'image Docker** :
```bash
# Sur x86_64/amd64 (Linux, Windows)
docker build -t runpod_comfyui .

# Sur Mac M1/M2/M3 (arm64)
docker buildx build --platform=linux/amd64 -t runpod_comfyui .
```

3. **Publiez l'image sur Docker Hub** :
```bash
docker tag runpod_comfyui votre-username/runpod_comfyui:latest
docker push votre-username/runpod_comfyui:latest
```

4. **DÃ©ployez sur RunPod** :
   - CrÃ©ez un template avec votre image Docker
   - Configurez la commande de dÃ©marrage : `/start.sh`
   - Exposez les ports HTTP : `8188,8888,3000`
   - Exposez les ports TCP : `22`
   - Ajoutez la variable d'environnement `HF_TOKEN` avec votre token Hugging Face

## ModÃ¨les inclus

### ðŸŽ¨ ModÃ¨les FLUX (GÃ©nÃ©ration d'images)
- **T5XXL (FP16)** - Encodeur de texte pour la gÃ©nÃ©ration d'embeddings textuels
- **CLIP-L** - Encodeur de texte CLIP Large pour la comprÃ©hension texte-image
- **FLUX VAE** - Autoencodeur variationnel pour la conversion latent/image
- **FLUX1-dev** - ModÃ¨le de diffusion FLUX1 (version dÃ©veloppeur)
- **FLUX1-Fill-dev** - ModÃ¨le FLUX pour le remplissage d'images (inpainting)
- **FLUX1-Depth-dev** - ModÃ¨le FLUX guidÃ© par la profondeur
- **FLUX1-Canny-dev** - ModÃ¨le FLUX guidÃ© par les contours Canny
- **FLUX1-Redux-dev** - ModÃ¨le FLUX pour la stylisation avancÃ©e

### âœ¨ NOUVEAUX MODÃˆLES (Mise Ã  jour 2025)
- **ðŸ–¼ï¸ FLUX.1-Kontext-dev** - Ã‰dition d'images contextuelle avec cohÃ©rence de personnage
- **ðŸ“¹ Wan 2.2 T2V A14B** - GÃ©nÃ©ration vidÃ©o Text-to-Video avec architecture MoE
- **ðŸ“¹ Wan 2.2 I2V A14B** - GÃ©nÃ©ration vidÃ©o Image-to-Video avec architecture MoE  
- **ðŸŒ Qwen-Image** - GÃ©nÃ©ration d'images multilingue (chinois/anglais)
- **ðŸŒ Qwen-Image-Edit** - Ã‰dition d'images multilingue avec texte prÃ©cis

### ðŸ” ModÃ¨les de support
- **Sigclip Vision** - ModÃ¨le de vision pour ComfyUI

> **ðŸ“š Documentation dÃ©taillÃ©e:** Consultez [NOUVEAUX_MODELES.md](docs/NOUVEAUX_MODELES.md) pour une description complÃ¨te des nouveaux modÃ¨les et de leurs fonctionnalitÃ©s.

## Organisation des rÃ©pertoires

```
/workspace/ComfyUI/              # RÃ©pertoire principal de ComfyUI
â”œâ”€â”€ custom_nodes/                # Extensions et nÅ“uds personnalisÃ©s
â”‚   â”œâ”€â”€ ComfyUI-Manager/         # Gestionnaire d'extensions
â”‚   â”œâ”€â”€ ComfyUI_TensorRT/        # Optimisations TensorRT
â”‚   â””â”€â”€ sdxl_prompt_styler/      # Stylisation de prompts pour SDXL
â”œâ”€â”€ input/                       # RÃ©pertoire pour les fichiers d'entrÃ©e
â”œâ”€â”€ models/                      # Tous les modÃ¨les
â”‚   â”œâ”€â”€ checkpoints/             # ModÃ¨les de base (checkpoint)
â”‚   â”œâ”€â”€ clip/                    # ModÃ¨les CLIP
â”‚   â”œâ”€â”€ clip_vision/             # ModÃ¨les CLIP Vision
â”‚   â”œâ”€â”€ controlnet/              # ModÃ¨les ControlNet
â”‚   â”œâ”€â”€ diffusion_models/        # ModÃ¨les de diffusion (FLUX, Qwen)
â”‚   â”œâ”€â”€ embeddings/              # Embeddings textuels
â”‚   â”œâ”€â”€ loras/                   # LoRA models
â”‚   â”œâ”€â”€ text_encoders/           # Encodeurs de texte
â”‚   â”œâ”€â”€ text_to_video/           # ModÃ¨les Text-to-Video (Wan 2.2)
â”‚   â”œâ”€â”€ image_to_video/          # ModÃ¨les Image-to-Video (Wan 2.2)
â”‚   â”œâ”€â”€ style_models/            # ModÃ¨les de style (Redux)
â”‚   â”œâ”€â”€ upscale_models/          # ModÃ¨les d'upscaling
â”‚   â””â”€â”€ vae/                     # ModÃ¨les VAE (FLUX, Wan, Qwen)
â””â”€â”€ output/                      # Images et vidÃ©os gÃ©nÃ©rÃ©es
```

## Optimisations pour RTX 5090

Cette image est spÃ©cialement optimisÃ©e pour tirer pleinement parti des GPU de derniÃ¨re gÃ©nÃ©ration comme la RTX 5090 :

- **Support d'architecture avancÃ©** - Prise en charge native des architectures CUDA 8.6, 9.0, 12.0 et 12.6
- **Optimisations TensorRT** - Conversion des modÃ¨les pour une infÃ©rence accÃ©lÃ©rÃ©e
- **Transferts HF optimisÃ©s** - Utilise le nouveau backend de transfert HF pour des tÃ©lÃ©chargements plus rapides
- **xFormers intÃ©grÃ©** - Optimisations d'attention pour rÃ©duire l'utilisation de VRAM et accÃ©lÃ©rer le traitement

## AccÃ¨s aux services

- **ComfyUI** : http://votre-pod-ip:3000
- **JupyterLab** : http://votre-pod-ip:8888 (si activÃ© via la variable `JUPYTER_PASSWORD`)
- **SSH** : `ssh root@votre-pod-ip` (mot de passe : runpod)

## Documentation complÃ¨te

Pour des instructions dÃ©taillÃ©es sur la configuration, l'utilisation et les fonctionnalitÃ©s avancÃ©es, consultez le [Guide d'utilisation](docs/GUIDE.md).

## Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  ouvrir une issue ou une pull request.

## Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

---

# ComfyUI for RunPod with CUDA 12.8 - RTX 5090 Compatible

This repository contains the files needed to create an optimized Docker container to run ComfyUI on the RunPod platform with CUDA 12.8, specially configured for the latest NVIDIA GPUs, including the RTX 5090.

## Features

- **CUDA 12.8 Base** - Full support for latest generation GPUs (RTX 5090, 4090, 4080, etc.)
- **NGC PyTorch 25.02 Image** - Uses official NVIDIA NGC image with PyTorch for maximum performance
- **Extended GPU Architecture** - Native support for compute capabilities 8.6, 9.0, 12.0, and 12.6
- **Automatic Model Download** - Models are downloaded from Hugging Face on first startup
- **Pre-installed Extensions** - ComfyUI-Manager, sdxl_prompt_styler, ComfyUI_TensorRT, and more
- **Web Interface with Nginx** - Optimized configuration for remote access
- **JupyterLab Support** - For development and feature exploration
- **Data Persistence** - Models and configurations are preserved in a network volume

## Prerequisites

- Docker installed on your machine (for building the image)
- If using a Mac with Apple Silicon (M1/M2/M3), use Docker Desktop with emulation support
- A RunPod account with credits or an available GPU
- A Hugging Face account (for model downloading)

## Quick Start

1. **Clone this repository**:
```bash
git clone https://github.com/Seven-94/Runpod_ComfyUI.git
cd Runpod_ComfyUI
```

2. **Build the Docker image**:
```bash
# On x86_64/amd64 (Linux, Windows)
docker build -t runpod_comfyui .

# On Mac M1/M2/M3 (arm64)
docker buildx build --platform=linux/amd64 -t runpod_comfyui .
```

3. **Publish the image to Docker Hub**:
```bash
docker tag runpod_comfyui your-username/runpod_comfyui:latest
docker push your-username/runpod_comfyui:latest
```

4. **Deploy on RunPod**:
   - Create a template with your Docker image
   - Configure the start command: `/start.sh`
   - Expose HTTP ports: `8188,8888,3000`
   - Expose TCP ports: `22`
   - Add the `HF_TOKEN` environment variable with your Hugging Face token

## Included Models

The following models are automatically downloaded from Hugging Face:
- **T5XXL (FP16)** - Text encoder for generating text embeddings
- **CLIP-L** - CLIP Large text encoder for text-image understanding
- **FLUX VAE** - Variational autoencoder for latent/image conversion
- **FLUX1-dev** - FLUX1 diffusion model (developer version)

## Directory Organization

```
/workspace/ComfyUI/              # Main ComfyUI directory
â”œâ”€â”€ custom_nodes/                # Extensions and custom nodes
â”‚   â”œâ”€â”€ ComfyUI-Manager/         # Extension manager
â”‚   â”œâ”€â”€ ComfyUI_TensorRT/        # TensorRT optimizations
â”‚   â””â”€â”€ sdxl_prompt_styler/      # Prompt styling for SDXL
â”œâ”€â”€ input/                       # Directory for input files
â”œâ”€â”€ models/                      # All models
â”‚   â”œâ”€â”€ checkpoints/             # Base models (checkpoint)
â”‚   â”œâ”€â”€ clip/                    # CLIP models
â”‚   â”œâ”€â”€ clip_vision/             # CLIP Vision models
â”‚   â”œâ”€â”€ controlnet/              # ControlNet models
â”‚   â”œâ”€â”€ diffusion_models/        # Diffusion models (like Flux)
â”‚   â”œâ”€â”€ embeddings/              # Text embeddings
â”‚   â”œâ”€â”€ loras/                   # LoRA models
â”‚   â”œâ”€â”€ text_encoders/           # Text encoders
â”‚   â”œâ”€â”€ upscale_models/          # Upscaling models
â”‚   â””â”€â”€ vae/                     # VAE models
â””â”€â”€ output/                      # Generated images
```

## RTX 5090 Optimizations

This image is specially optimized to take full advantage of the latest generation GPUs like the RTX 5090:

- **Advanced Architecture Support** - Native support for CUDA architectures 8.6, 9.0, 12.0, and 12.6
- **TensorRT Optimizations** - Model conversion for accelerated inference
- **Optimized HF Transfers** - Uses the new HF transfer backend for faster downloads
- **Built-in xFormers** - Attention optimizations to reduce VRAM usage and speed up processing

## Accessing Services

- **ComfyUI**: http://your-pod-ip:3000
- **JupyterLab**: http://your-pod-ip:8888 (if enabled via the `JUPYTER_PASSWORD` variable)
- **SSH**: `ssh root@your-pod-ip` (password: runpod)

## Complete Documentation

For detailed instructions on configuration, usage, and advanced features, check the [User Guide](docs/GUIDE.md).

## Contributing

Contributions are welcome! Feel free to open an issue or pull request.

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.